{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0a2158-210d-41dd-a514-cbbc04b44021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 3 Task 1 By Kevin Wong\n",
    "#CS4210 Machine Learning and its Applications - Summer Semester 2024\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae881acf-358f-4b68-89e7-6fe7581d5ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Determine if CUDA (GPU) is available and set the device accordingly\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db58acc-aa0b-4fc4-a97a-e8aae08df65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "# Define transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "    transforms.RandomHorizontalFlip(),  # Apply random horizontal flip\n",
    "    transforms.RandomRotation(5),  # Apply random rotation\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image pixel values to have mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Define transformations for validation and test data (only normalization)\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Custom dataset class to handle image data and apply transforms\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image) # Apply the specified transformations to the image\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label # Return both the image and the corresponding label\n",
    "        return image # For test data, only return the image\n",
    "\n",
    "\n",
    "# Load datasets from CSV files\n",
    "train_data = pd.read_csv('train_data.csv', header=None)\n",
    "train_target = pd.read_csv('train_target.csv', header=None)\n",
    "test_data = pd.read_csv('test_data.csv', header=None)\n",
    "\n",
    "# Reshape the data (each row is 2304 pixels, reshape to 48x48 images)\n",
    "train_data = train_data.values.reshape(-1, 48, 48).astype('float32')\n",
    "test_data = test_data.values.reshape(-1, 48, 48).astype('float32')\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_data, val_data, train_target, val_target = train_test_split(\n",
    "    train_data, train_target.values.flatten(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to torch tensors and apply transforms\n",
    "train_dataset = CustomImageDataset(train_data, train_target, transform=transform_train)\n",
    "val_dataset = CustomImageDataset(val_data, val_target, transform=transform_val_test)\n",
    "test_dataset = CustomImageDataset(test_data, transform=transform_val_test)\n",
    "\n",
    "# Create DataLoader objects for batching and shuffling the data during training and evaluation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a54e5e-059a-414b-acde-a756437a05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Building the Model\n",
    "\n",
    "# Define a Convolutional Neural Network (CNN) architecture for image classification\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        # Fully connected layers with dropout for regularization\n",
    "        self.fc1 = nn.Linear(512*3*3, 512)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 3)  # 3 classes: Angry, Happy, Neutral\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass using Leaky ReLU activation functions and max pooling\n",
    "        x = F.leaky_relu(F.max_pool2d(self.bn1(self.conv1(x)), 2), negative_slope=0.01)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.bn2(self.conv2(x)), 2), negative_slope=0.01)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.bn3(self.conv3(x)), 2), negative_slope=0.01)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.bn4(self.conv4(x)), 2), negative_slope=0.01)\n",
    "        x = x.view(-1, 512*3*3) # Flatten the output from the convolutional layers\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
    "        x = self.dropout(x)  # Apply dropout for regularization\n",
    "        x = self.fc2(x) # Final output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce60501-515b-442b-b78f-c5e3d01a15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.2308, Validation Loss: 0.9735, Validation Accuracy: 53.57%\n",
      "Epoch [2/20], Loss: 0.8160, Validation Loss: 0.6824, Validation Accuracy: 70.17%\n",
      "Epoch [3/20], Loss: 0.6942, Validation Loss: 0.6173, Validation Accuracy: 73.48%\n",
      "Epoch [4/20], Loss: 0.6406, Validation Loss: 0.6376, Validation Accuracy: 72.02%\n",
      "Epoch [5/20], Loss: 0.6043, Validation Loss: 0.6544, Validation Accuracy: 73.04%\n",
      "Epoch [6/20], Loss: 0.5716, Validation Loss: 0.5588, Validation Accuracy: 76.88%\n",
      "Epoch [7/20], Loss: 0.5473, Validation Loss: 0.5569, Validation Accuracy: 76.54%\n",
      "Epoch [8/20], Loss: 0.5283, Validation Loss: 0.5575, Validation Accuracy: 77.90%\n",
      "Epoch [9/20], Loss: 0.4996, Validation Loss: 0.5639, Validation Accuracy: 75.89%\n",
      "Epoch [10/20], Loss: 0.4869, Validation Loss: 0.5692, Validation Accuracy: 77.19%\n",
      "Epoch [11/20], Loss: 0.4695, Validation Loss: 0.5496, Validation Accuracy: 77.37%\n",
      "Epoch [12/20], Loss: 0.4588, Validation Loss: 0.5665, Validation Accuracy: 77.50%\n",
      "Epoch [13/20], Loss: 0.4274, Validation Loss: 0.6249, Validation Accuracy: 74.37%\n",
      "Epoch [14/20], Loss: 0.4151, Validation Loss: 0.5496, Validation Accuracy: 77.43%\n",
      "Epoch [15/20], Loss: 0.3340, Validation Loss: 0.5197, Validation Accuracy: 80.68%\n",
      "Epoch [16/20], Loss: 0.2819, Validation Loss: 0.5193, Validation Accuracy: 81.02%\n",
      "Epoch [17/20], Loss: 0.2624, Validation Loss: 0.5346, Validation Accuracy: 80.80%\n",
      "Epoch [18/20], Loss: 0.2416, Validation Loss: 0.5460, Validation Accuracy: 80.77%\n",
      "Epoch [19/20], Loss: 0.2208, Validation Loss: 0.5600, Validation Accuracy: 81.11%\n",
      "Epoch [20/20], Loss: 0.2093, Validation Loss: 0.5585, Validation Accuracy: 81.11%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define loss function and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss() # Use cross-entropy loss for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4) # Use Adam optimizer with weight decay\n",
    "\n",
    "# Step 3: Training the Model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate the loss\n",
    "            loss.backward() # Backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clip gradients to avoid exploding gradients\n",
    "            optimizer.step() # Update the weights\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion) # Validate the model on the validation set\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate the loss\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1) # Get the predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item() # Calculate the number of correct predictions\n",
    "    \n",
    "    val_loss /= len(val_loader) # Calculate average validation loss\n",
    "    val_accuracy = 100 * correct / total # Calculate validation accuracy\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c43692-6816-4a69-a87e-36f7238ae733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Making Predictions\n",
    "\n",
    "def make_predictions(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for images in test_loader:\n",
    "            outputs = model(images) # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1) # Get the predicted class\n",
    "            predictions.extend(predicted.cpu().numpy())  # Store the predictions\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions = make_predictions(model, test_loader)\n",
    "\n",
    "# Prepare submission file\n",
    "submission_df = pd.DataFrame({'Id': np.arange(len(predictions)), 'Category': predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f855420-f385-4630-afb5-65f0724c6865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83353aa7-49d8-4b6b-a24d-5908d83276e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
